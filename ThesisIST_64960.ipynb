{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-15 04:17:48,044] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q agente iniciado\n",
      "Q agente iniciado\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy.matlib as npmat\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import threading\n",
    "\n",
    "def toggle_rendering():\n",
    "\n",
    "    global render\n",
    "    global runflag\n",
    "    while runflag:\n",
    "        p = raw_input()\n",
    "        if p == 'q':\n",
    "            runflag = False\n",
    "        else:\n",
    "            render = not render\n",
    "            \n",
    "            \n",
    "            \n",
    "class Q_agente_DNN(object):   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l=0.0,\n",
    "                 momentum=0.2):\n",
    "        \n",
    "        \n",
    "        # Sessão de serviço para correr o grafo\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # Dimensões da rede\n",
    "        self.n_input    = dim_input      #dimensão do input\n",
    "        self.n_hidden_1 = dim_hidden1    #dimensão da primeira layer\n",
    "        self.n_hidden_2 = dim_hidden2    #dimensão da segunda layer\n",
    "        self.n_output   = n_accao        #número de acções, igual à dimensão do output da rede\n",
    "       \n",
    "        # Parâmetros de Aprendizagem, relacionados com o treino ou a função de custo\n",
    "        self.learning_rate   = learning_rate                #learning rate do optimizador \n",
    "        self.gamma           = gamma                        #desconto de rewards futuras\n",
    "        self.epsilon         = epsilon                      #probabilidade de escolher uma acção random\n",
    "        \n",
    "        # Constante do regularizador da função objectiva\n",
    "        self.reg = 0.01  \n",
    "        \n",
    "        # Variáveis de percepcção e actuação do agente, inclui observação, acção e reward correspondente\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input])  # Perfil do tensor de observação, contêm a dimensão do input\n",
    "        self.r = tf.placeholder(\"float\", [None, 1])             # Reward obtida\n",
    "        self.a = tf.placeholder(\"float\", [None, n_accao])       # Acção exercida pelo agente \n",
    "        self.future_rewards = tf.placeholder(\"float\", [None])   # No futuro usar tf.Variable_Scope para lidar de forma mais segura com tf.Variables\n",
    "        print (\"Q agente iniciado\")\n",
    "        \n",
    "        # Dicionários com os pesos e enviasamento da rede\n",
    "        self.stddev = 0.1\n",
    "        self.pesos = {\n",
    "            'h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1], stddev=self.stddev)),\n",
    "            'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev=self.stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.n_output], stddev=self.stddev))\n",
    "        }\n",
    "        self.enviasamentos = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden_1],stddev=self.stddev)),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden_2],stddev=self.stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_output],stddev=self.stddev))\n",
    "        }\n",
    "    \n",
    "        # Rede Q: dim_input---»dim_h(RELU)---»dim_h(RELU)---»dim_output(LINEAR)\n",
    "    def Q_rede(self,_X, _pesos, _enviasamentos):\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _pesos['h1']), _enviasamentos['b1'])) \n",
    "        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _pesos['h2']), _enviasamentos['b2']))\n",
    "        output  = tf.matmul(layer_2, _pesos['out']) + _enviasamentos['out']\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def Q_agente_initializar(self):\n",
    "        # Definir função objectiva, optimizador\n",
    "       \n",
    "        self.pred = self.Q_rede(self.x, self.pesos, self.enviasamentos)\n",
    "        self.y  = tf.reduce_sum(tf.mul(self.pred,self.a), reduction_indices = 1)\n",
    "        \n",
    "        # Se quisermos incluir regularização, embora o batch aleatório de samples funcione como regularizador\n",
    "        #weightL2loss = self.reg*tf.nn.l2_loss(self.pesos['h1']) + self.reg*tf.nn.l2_loss(self.pesos['h2']) + self.reg*tf.nn.l2_loss(self.pesos['out'])\n",
    "        #biasL2loss = self.reg*tf.nn.l2_loss(self.enviasamentos['b1']) + self.reg*tf.nn.l2_loss(self.enviasamentos['b1']) + self.reg*tf.nn.l2_loss(self.enviasamentos['out'])\n",
    "        #self.future_rewards = tf.add(self.r,tf.mul(self.gamma,self.q_target))\n",
    "        \n",
    "        # Função objectiva e optimizador\n",
    "        self.cost = tf.reduce_mean(tf.square(self.future_rewards - self.y)) #+ weightL2loss + biasL2loss\n",
    "        #self.cost = tf.reduce_mean(tf.square(self.future_rewards - self.y)) + weightL2loss + biasL2loss#+ weightL2loss + biasL2loss\n",
    "        self.optm = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost) # Adam Optimizer\n",
    "\n",
    "        # Initializar todas as variáveis (i.e, ligar e initializar os nós do grafo) e a sessão de trabalho\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    \n",
    "    def actua(self,obs):\n",
    "        \n",
    "        accao = np.argmax(self.sess.run(self.pred, feed_dict={self.x: obs}))\n",
    "        \n",
    "        return accao\n",
    "\n",
    "        \n",
    "    \n",
    "    # Classe da target Network, herda a Q_network e obtêm mais um método para copiar pesos. Foi criada em separado para existir numa sessão completamente\n",
    "    # diferente (conflictos de variáveis)\n",
    "class Target_Q_agente_DNN(Q_agente_DNN):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l=0.0,\n",
    "                 momentum=0.2):\n",
    "        super(Target_Q_agente_DNN, self).__init__(dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l,\n",
    "                 momentum)\n",
    "        \n",
    "    # Copiar pesos de outra sessão para esta\n",
    "    def copiar_pesos(self,q_network):\n",
    "        \n",
    "        self.op1=self.pesos['h1'].assign(q_network.pesos['h1'].eval(session = q_network.sess))\n",
    "        self.op2=self.pesos['h2'].assign(q_network.pesos['h2'].eval(session = q_network.sess))\n",
    "        self.op3=self.pesos['out'].assign(q_network.pesos['out'].eval(session = q_network.sess))\n",
    "        \n",
    "        self.op4=self.enviasamentos['b1'].assign(q_network.enviasamentos['b1'].eval(session = q_network.sess))\n",
    "        self.op5=self.enviasamentos['b2'].assign(q_network.enviasamentos['b2'].eval(session = q_network.sess))\n",
    "        self.op6=self.enviasamentos['out'].assign(q_network.enviasamentos['out'].eval(session = q_network.sess))\n",
    "    \n",
    "        self.sess.run(self.op1)\n",
    "        self.sess.run(self.op2)\n",
    "        self.sess.run(self.op3)\n",
    "        self.sess.run(self.op4)\n",
    "        self.sess.run(self.op5)\n",
    "        self.sess.run(self.op6)\n",
    "   \n",
    "\n",
    "\"\"\"Parâmetros de simulação\"\"\"\n",
    "dim_accao = 3 # Número de acções, 2 para o CartPole-v0\n",
    "dim_input = 2 # Posição e velocidade, no CartPole-v0 são 4\n",
    "dim_h = 100  # Dimensão das hidden layers, iremos usar o mesmo número para ambas\n",
    "episodes = 1000  # Número de episódios\n",
    "time_lim = 10000  # Número máximo de steps\n",
    "problem = 'MountainCar-v0'\n",
    "# problem = 'CartPole-v0'\n",
    "gamma = 0.99  # Desconto de Rewards futuras\n",
    "alpha = 0.0075  # Learning rate\n",
    "target_update = 1000  # número de steps entre updates da target network\n",
    "REPLAY_MEMORY = 500000 #replay memory\n",
    "avgRh = []  # Vector para efectuar a média de Rw\n",
    "avVfh = []  # Vector para efectuar a média de Vfh\n",
    "Qh = []  # Vector para armazenar Q-values de estado inicial\n",
    "Vfh = [] # Vector para armazenar Values do estado final\n",
    "Rw = deque([], 100)  # Últimas 100 rewards acumuladas\n",
    "Vf = deque([], 100)  # Últimos 100 Values no estado terminal\n",
    "render = False  # Render da simulação\n",
    "render_last = 2  # Render dos últimos steps da simulação\n",
    "batch_size = 8  # Size dos batchs\n",
    "epsilon = 0.5  # Epsilon inicial\n",
    "eps_dec = 0.995 # Taxa decrescente de epsilon\n",
    "eps_min = 0.005 # Epsilon mínimo\n",
    "rdeq = deque([], 100) # Rewards acumuladas por step\n",
    "max_costs_hist = []\n",
    "min_costs_hist = []\n",
    "med_costs_hist = []\n",
    "\n",
    "\"\"\" Simulação\"\"\"\n",
    "env = gym.make(problem)\n",
    "Q=Q_agente_DNN(dim_input,dim_h,dim_h,dim_accao,gamma,0.5,0.0075)\n",
    "T=Target_Q_agente_DNN(dim_input,dim_h,dim_h,dim_accao,gamma,0.5,0.0075)\n",
    "Q.Q_agente_initializar()\n",
    "T.Q_agente_initializar()\n",
    "D = deque()\n",
    "tr = threading.Thread(target=toggle_rendering)\n",
    "runflag = True\n",
    "tr.start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Episódio 0 Completo ===\n",
      "Avg r:  -3305.0\n",
      "Instant sumR:  -3305.0\n",
      "=== Episódio 1 Completo ===\n",
      "Avg r:  -1949.5\n",
      "Instant sumR:  -594.0\n",
      "=== Episódio 2 Completo ===\n",
      "Avg r:  -2016.0\n",
      "Instant sumR:  -2149.0\n",
      "=== Episódio 3 Completo ===\n",
      "Avg r:  -4012.0\n",
      "Instant sumR:  -10000.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for e in xrange(episodes):\n",
    "        \n",
    "        \n",
    "        #if e==1000 or e==1500 or e==1800 or e==2500 or e==2800:\n",
    "        #    print \"gravar\"\n",
    "        #    env.monitor.start('/tmp/MountainCar-v0-'+str(e))\n",
    "        \n",
    "        o = env.reset() \n",
    "        oprev = np.array(o, dtype='float32')\n",
    "        qi = Q.pred.eval(session=Q.sess,feed_dict = {Q.x : [oprev]})\n",
    "        a = Q.actua([oprev])\n",
    "        a_t = np.zeros([dim_accao])\n",
    "        a_t[0] = 1\n",
    "        sumR = 0\n",
    "        costs_hist = []\n",
    "       \n",
    "            \n",
    "        for t in xrange(time_lim):\n",
    "\n",
    "            if not runflag:\n",
    "                break\n",
    "\n",
    "            if render or episodes-e < render_last:\n",
    "                env.render()\n",
    "\n",
    "            (o, r, done, info) = env.step(a)\n",
    "            onext = np.array(o, dtype='float32')\n",
    "            \n",
    "            if done or t==time_lim-1:\n",
    "                of = oprev\n",
    "            rdeq.append(r)\n",
    "\n",
    "            D.append((oprev,a_t,onext,r,done))\n",
    "            \n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft() \n",
    "            \n",
    "            if (t+1) % target_update == 0:\n",
    "                T.copiar_pesos(Q)\n",
    "            \n",
    "            a_t = np.zeros([dim_accao])\n",
    "\n",
    "            \n",
    "            \"\"\" Treinar a Qnet \"\"\"\n",
    "            if len(D) >= batch_size:\n",
    "                # Sample de batches\n",
    "                batch = random.sample(D, batch_size)\n",
    "\n",
    "                s_t_batch = [d[0] for d in batch]\n",
    "                a_batch = [d[1] for d in batch]\n",
    "                s_t_1_batch = [d[2] for d in batch]\n",
    "                r_batch = [d[3] for d in batch]\n",
    "                \n",
    "                # Preparar targets com a target Q-Network\n",
    "                t_batch = []\n",
    "                target_t1_batch = T.pred.eval(session=T.sess,feed_dict = {T.x : s_t_1_batch})\n",
    "                for i in range(0, len(batch)):\n",
    "                    if batch[i][4]:\n",
    "                        t_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        t_batch.append(r_batch[i] + gamma * np.max(target_t1_batch[i]))\n",
    "                \n",
    "               \n",
    "                costs_hist.append(Q.sess.run(Q.cost,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch}))\n",
    "                \n",
    "                # Apenas um step \n",
    "                Q.sess.run(Q.optm,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch})\n",
    "                \n",
    "\n",
    "                if random.random() <= epsilon:\n",
    "                    a = random.randrange(dim_accao)\n",
    "                    a_t[a] = 1\n",
    "                else:\n",
    "                    a = Q.actua([onext])\n",
    "                    a_t[a] = 1\n",
    "                    \n",
    "            else:\n",
    "                a = np.random.choice(dim_accao)\n",
    "                a_t[a] = 1\n",
    "            sumR += r\n",
    "            oprev = onext\n",
    "            if done:\n",
    "                break\n",
    "        Rw.append(sumR)\n",
    "        avgRh.append(np.mean(Rw))\n",
    "        Vf.append(np.max(Q.pred.eval(session=Q.sess,feed_dict = {Q.x : [of]})))\n",
    "        avVfh.append(np.mean(Vf))\n",
    "        Qh.append(qi[0])\n",
    "        epsilon = epsilon*eps_dec if epsilon > eps_min else eps_min\n",
    "        max_costs_hist.append(np.max(costs_hist))\n",
    "        min_costs_hist.append(np.min(costs_hist))\n",
    "        med_costs_hist.append(np.median(costs_hist))\n",
    "        #if e==1000 or e==1500 or e==1800 or e==2500 or e==2800:\n",
    "        #    env.monitor.close()\n",
    "        print \"=== Episódio %d Completo ===\" %e\n",
    "        print \"Avg r: \", np.mean(Rw)\n",
    "        print \"Instant sumR: \", sumR\n",
    "\n",
    "        if not runflag:\n",
    "            break\n",
    "    \n",
    "    print \"=== Simulação Completa ===\"\n",
    "    print \"Score Final: %d\" % avgRh[-1]\n",
    "\n",
    "    \"\"\" Visualization of the results \"\"\"\n",
    "    f1 = plt.figure()\n",
    "    spy = f1.add_subplot(111)\n",
    "    spy.plot(avgRh,'g',label='Avg. R over 100 eps.', linewidth=2.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Acc. R over last 100 eps')\n",
    "    \n",
    "    f2 = plt.figure()\n",
    "    spy = f2.add_subplot(111)\n",
    "    Qhnp = np.asmatrix(Qh)\n",
    "    spy.plot(Qhnp[:, 0], 'b', label='Q(s0, left)', linewidth=2.0)\n",
    "    spy.plot(Qhnp[:, 1], 'm', label='Q(s0, none)', linewidth=2.0)\n",
    "    spy.plot(Qhnp[:, 2], 'k', label='Q(s0, right)', linewidth=2.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Q-values')\n",
    "    \n",
    "    f3 = plt.figure()\n",
    "    spy = f3.add_subplot(111)\n",
    "    spy.plot(max_costs_hist, 'r', label='max cost', linewidth=2.0)\n",
    "    spy.plot(min_costs_hist, 'k', label='min cost', linewidth=2.0)\n",
    "    spy.plot(med_costs_hist, 'm', label='med cost', linewidth=2.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cost Function')\n",
    "    \n",
    "    f4 = plt.figure()\n",
    "    spy = f4.add_subplot(111)\n",
    "    spy.plot(avVfh, label='Value at final state', linewidth=2.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average (last 100) Values at final state')\n",
    "    \n",
    "    # Fechar sessões\n",
    "    #Q.sess.close()\n",
    "    #T.sess.close()\n",
    "    \n",
    "    plt.show()\n",
    "except KeyboardInterrupt:\n",
    "    runflag = False\n",
    "    tr.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_grad = tf.gradients(Q.cost,Q.y)\n",
    "var_grad2 = tf.gradients(Q.cost,Q.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = Q.sess.run(var_grad,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch})\n",
    "c= Q.sess.run(var_grad2,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00175682, -0.00113903,  0.00285447,  0.00149781,  0.00014873,\n",
       "        -0.00339086, -0.00207981,  0.00334351], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00175682,  0.        ,  0.        ],\n",
       "        [-0.        , -0.        , -0.00113903],\n",
       "        [ 0.00285447,  0.        ,  0.        ],\n",
       "        [ 0.00149781,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.00014873,  0.        ],\n",
       "        [-0.00339086, -0.        , -0.        ],\n",
       "        [-0.        , -0.00207981, -0.        ],\n",
       "        [ 0.00334351,  0.        ,  0.        ]], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.,  0.,  0.]),\n",
       " array([ 0.,  0.,  1.]),\n",
       " array([ 1.,  0.,  0.]),\n",
       " array([ 1.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 1.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 1.,  0.,  0.])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.35999e-05\n"
     ]
    }
   ],
   "source": [
    "cost=Q.sess.run(Q.cost,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch})\n",
    "print cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy= Q.sess.run(Q.y,feed_dict = {\n",
    "                    Q.future_rewards : t_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_t_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.85418957 -0.8674137  -0.84828264 -0.85512346 -0.85979676 -0.87846535\n",
      " -0.86971986 -0.84609586]\n"
     ]
    }
   ],
   "source": [
    "print yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.86121686622500415, -0.86285760030150416, -0.8597005075216293, -0.86111470758914943, -0.86039168998599047, -0.86490190893411634, -0.8614006337523461, -0.85946987241506578]\n"
     ]
    }
   ],
   "source": [
    "print t_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00702729  0.0045561  -0.01141787 -0.00599125 -0.00059493  0.01356345\n",
      "  0.00831923 -0.01337401]\n"
     ]
    }
   ],
   "source": [
    "aux=t_batch-yy\n",
    "print aux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
