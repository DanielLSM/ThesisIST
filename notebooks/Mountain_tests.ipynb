{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Init\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy.matlib as npmat\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import threading\n",
    "#from Q_agente_modelo import Q_agente_DNN, Target_Q_agente_DNN, toggle_rendering\n",
    "\n",
    "\"\"\" Parameters \"\"\"\n",
    "def toggle_rendering():\n",
    "    \"\"\"\n",
    "    This is just a helper function to toggle the environment\n",
    "    rendering or quit the simulation\n",
    "    \"\"\"\n",
    "    global render\n",
    "    global runflag\n",
    "    while runflag:\n",
    "        p = raw_input()\n",
    "        if p == 'q':\n",
    "            runflag = False\n",
    "        else:\n",
    "            render = not render\n",
    "            \n",
    "            \n",
    "            \n",
    "class Q_agente_DNN(object):   \n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l=0.0,\n",
    "                 momentum=0.2):\n",
    "        \n",
    "        # Sessão de serviço para correr o grafo\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # Dimensões da rede\n",
    "        self.n_input    = dim_input      #dimensão do input\n",
    "        self.n_hidden_1 = dim_hidden1    #dimensão da primeira layer\n",
    "        self.n_hidden_2 = dim_hidden2    #dimensão da segunda layer\n",
    "        self.n_output   = n_accao        #número de acções, igual à dimensão do output da rede\n",
    "       \n",
    "        # Parâmetros de Aprendizagem, relacionados com o treino ou a função de custo\n",
    "        self.learning_rate   = learning_rate                #learning rate do optimizador \n",
    "        self.gamma           = gamma                        #desconto de rewards futuras\n",
    "        self.epsilon         = epsilon                      #probabilidade de escolher uma acção random\n",
    "        \n",
    "        # RIGGED\n",
    "        #self.reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.reg = 0.01  # Choose an appropriate one.\n",
    "        \n",
    "        # Variáveis de percepcção e actuação do agente, inclui observação, acção e reward correspondente\n",
    "        # De notar que não precisamos da observação seguinte, visto estarmos a efectuar Q-learning em ambiente deterministico\n",
    "        # acção será o argmax do Q_agente_target\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input])  # Perfil do tensor de observação, contêm a dimensão do input\n",
    "        self.r = tf.placeholder(\"float\", [None, 1])             # Reward obtida\n",
    "        self.q_target = tf.placeholder(\"float\", [None, 1]) \n",
    "   \n",
    "        self.a = tf.placeholder(\"float\", [None, n_accao])             # Acção exercida pelo agente \n",
    "        self.future_rewards = tf.placeholder(\"float\", [None])\n",
    "        # No futuro usar tf.Variable_Scope para lidar de forma mais segura com tf.Variables\n",
    "        # Dicionários com os pesos e biases da rede\n",
    "        self.stddev = 0.1 # <== This greatly affects accuracy!! \n",
    "        self.weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1], stddev=self.stddev)),\n",
    "            'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev=self.stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.n_output], stddev=self.stddev))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden_1],stddev=self.stddev)),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden_2],stddev=self.stddev)),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_output],stddev=self.stddev))\n",
    "        }\n",
    "    \n",
    "        # Create model\n",
    "    def multilayer_perceptron(self,_X, _weights, _biases):\n",
    "        layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2']))\n",
    "        #output  = tf.nn.relu(tf.matmul(layer_2, _weights['out']) + _biases['out'])\n",
    "        output  = tf.matmul(layer_2, _weights['out']) + _biases['out']\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def construct_model_and_initialize(self):\n",
    "        # Construct model\n",
    "\n",
    "        \n",
    "        self.pred = self.multilayer_perceptron(self.x, self.weights, self.biases)\n",
    "        self.y  = tf.reduce_sum(tf.mul(self.pred,self.a), reduction_indices = 1)\n",
    "        #weightL2loss = self.reg*tf.nn.l2_loss(self.weights['h1']) + self.reg*tf.nn.l2_loss(self.weights['h2']) + self.reg*tf.nn.l2_loss(self.weights['out'])\n",
    "        #biasL2loss = self.reg*tf.nn.l2_loss(self.biases['b1']) + self.reg*tf.nn.l2_loss(self.biases['b1']) + self.reg*tf.nn.l2_loss(self.biases['out'])\n",
    "        #self.future_rewards = tf.add(self.r,tf.mul(self.gamma,self.q_target))\n",
    "        \n",
    "        # Define loss and optimizer\n",
    "        self.cost = tf.reduce_mean(tf.square(self.future_rewards - self.y)) #+ weightL2loss + biasL2loss\n",
    "        self.optm = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost) # Adam Optimizer\n",
    "\n",
    "        # Initializing the variables\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.sess.run(self.init)\n",
    "        print (\"Network Ready And Initialized\")\n",
    "    \n",
    "    def actua(self,obs):\n",
    "        \n",
    "        accao = np.argmax(self.sess.run(self.pred, feed_dict={self.x: obs}))\n",
    "        \n",
    "        return accao\n",
    "    \n",
    "   \n",
    "    #def copiar_Q_agente(self):\n",
    "    #    with tf.name_scope(\"target_network_update\"):\n",
    "    #        self.target_network_update = []\n",
    "    #        for v_source, v_target in zip(self.q_network.variables(), self.target_q_network.variables()):\n",
    "    #            # this is equivalent to target = (1-alpha) * target + alpha * source\n",
    "    #            update_op = v_target.assign_sub(self.target_network_update_rate * (v_target - v_source))\n",
    "    #            self.target_network_update.append(update_op)\n",
    "    #        self.target_network_update = tf.group(*self.target_network_update)\n",
    "        \n",
    "    print(\"Agent Init\")\n",
    "    \n",
    "class Target_Q_agente_DNN(Q_agente_DNN):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l=0.0,\n",
    "                 momentum=0.2):\n",
    "        super(Target_Q_agente_DNN, self).__init__(dim_input,      \n",
    "                 dim_hidden1,    \n",
    "                 dim_hidden2,    \n",
    "                 n_accao,        \n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 learning_rate,  \n",
    "                 regularisation_l,\n",
    "                 momentum)\n",
    "    # Definir aqui todas as operações de updates de variáveis. Initializar variáveis em operações recurrentes,\n",
    "    # faz com que sejam recursivamente adicionadas novas variáveis com as mesmas funções. \n",
    "    def computacao_eficiente(self,q_network):\n",
    "        \n",
    "        self.op1=self.weights['h1'].assign(q_network.weights['h1'].eval(session = q_network.sess))\n",
    "        self.op2=self.weights['h2'].assign(q_network.weights['h2'].eval(session = q_network.sess))\n",
    "        self.op3=self.weights['out'].assign(q_network.weights['out'].eval(session = q_network.sess))\n",
    "        \n",
    "        self.op4=self.biases['b1'].assign(q_network.biases['b1'].eval(session = q_network.sess))\n",
    "        self.op5=self.biases['b2'].assign(q_network.biases['b2'].eval(session = q_network.sess))\n",
    "        self.op6=self.biases['out'].assign(q_network.biases['out'].eval(session = q_network.sess))\n",
    "    \n",
    "        self.sess.run(self.op1)\n",
    "        self.sess.run(self.op2)\n",
    "        self.sess.run(self.op3)\n",
    "        self.sess.run(self.op4)\n",
    "        self.sess.run(self.op5)\n",
    "        self.sess.run(self.op6)\n",
    "    #def copiar_pesos(self,q_network):\n",
    "        \n",
    "    #    self.sess.run(self.op1)\n",
    "    #    self.sess.run(self.op2)\n",
    "    #    self.sess.run(self.op3)\n",
    "    #    self.sess.run(self.op4)\n",
    "    #    self.sess.run(self.op5)\n",
    "    #    self.sess.run(self.op6)\n",
    "        #self.sess.run(self)\n",
    "        #print q_network.weights['h1'].eval(session = q_network.sess)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "nA = 3\n",
    "episodes = 100  # number of episodes to run\n",
    "time_lim = 20000  # max number of steps per episode\n",
    "problem = 'MountainCar-v0'\n",
    "gamma = 0.99  # discount factor\n",
    "alpha = 0.0075  # learning rate\n",
    "C = 1500  # target net update frequency\n",
    "N = 500000  # maximum number of experiences stored in memory\n",
    "REPLAY_MEMORY = 500000 #replay memory\n",
    "avgRh = []  # to store average rewards for visualization\n",
    "Qh = []  # to store value from the initial state for visualization\n",
    "Vfh = []\n",
    "Rw = deque([], 100)  # running history of rewards\n",
    "render = False  # render the simulation\n",
    "render_last = 2  # render the last N steps of the simulation\n",
    "dim_h = 100  # width of each hidden layer  (best = 5)\n",
    "batch_size = 8  # minibatch size  (best = 64)\n",
    "epsilon = 0.5\n",
    "eps_dec = 0.995\n",
    "eps_min = 0.1\n",
    "rdeq = deque([], 100)\n",
    "max_costs_hist = []\n",
    "min_costs_hist = []\n",
    "med_costs_hist = []\n",
    "H = npmat.zeros([7, N], dtype='float32')\n",
    "hi = 0\n",
    "hsize = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-24 21:35:50,195] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready And Initialized\n",
      "Network Ready And Initialized\n",
      "=== Episode 0 complete ===\n",
      "Avg r:  -20000.0\n",
      "Instant sumR:  -20000.0\n",
      "=== Episode 1 complete ===\n",
      "Avg r:  -12854.5\n",
      "Instant sumR:  -5709.0\n",
      "=== Episode 2 complete ===\n",
      "Avg r:  -10564.0\n",
      "Instant sumR:  -5983.0\n",
      "=== Episode 3 complete ===\n",
      "Avg r:  -9679.75\n",
      "Instant sumR:  -7027.0\n",
      "=== Episode 4 complete ===\n",
      "Avg r:  -10210.2\n",
      "Instant sumR:  -12332.0\n",
      "=== Episode 5 complete ===\n",
      "Avg r:  -8685.0\n",
      "Instant sumR:  -1059.0\n",
      "=== Episode 6 complete ===\n",
      "Avg r:  -10301.4285714\n",
      "Instant sumR:  -20000.0\n",
      "=== Episode 7 complete ===\n",
      "Avg r:  -11505.0\n",
      "Instant sumR:  -19930.0\n",
      "=== Episode 8 complete ===\n",
      "Avg r:  -12448.8888889\n",
      "Instant sumR:  -20000.0\n",
      "=== Episode 9 complete ===\n",
      "Avg r:  -12744.1\n",
      "Instant sumR:  -15401.0\n",
      "=== Episode 10 complete ===\n",
      "Avg r:  -12160.6363636\n",
      "Instant sumR:  -6326.0\n",
      "=== Episode 11 complete ===\n",
      "Avg r:  -12813.9166667\n",
      "Instant sumR:  -20000.0\n",
      "=== Episode 12 complete ===\n",
      "Avg r:  -12965.4615385\n",
      "Instant sumR:  -14784.0\n",
      "=== Episode 13 complete ===\n",
      "Avg r:  -12234.3571429\n",
      "Instant sumR:  -2730.0\n",
      "=== Episode 14 complete ===\n",
      "Avg r:  -11725.7333333\n",
      "Instant sumR:  -4605.0\n",
      "=== Episode 15 complete ===\n",
      "Avg r:  -11133.1875\n",
      "Instant sumR:  -2245.0\n",
      "=== Episode 16 complete ===\n",
      "Avg r:  -10531.0588235\n",
      "Instant sumR:  -897.0\n",
      "=== Episode 17 complete ===\n",
      "Avg r:  -10010.3888889\n",
      "Instant sumR:  -1159.0\n",
      "=== Episode 18 complete ===\n",
      "Avg r:  -9798.73684211\n",
      "Instant sumR:  -5989.0\n",
      "=== Episode 19 complete ===\n",
      "Avg r:  -9459.7\n",
      "Instant sumR:  -3018.0\n",
      "=== Episode 20 complete ===\n",
      "Avg r:  -9128.14285714\n",
      "Instant sumR:  -2497.0\n",
      "=== Episode 21 complete ===\n",
      "Avg r:  -8917.04545455\n",
      "Instant sumR:  -4484.0\n",
      "=== Episode 22 complete ===\n",
      "Avg r:  -8611.56521739\n",
      "Instant sumR:  -1891.0\n",
      "=== Episode 23 complete ===\n",
      "Avg r:  -8345.20833333\n",
      "Instant sumR:  -2219.0\n",
      "=== Episode 24 complete ===\n",
      "Avg r:  -8107.12\n",
      "Instant sumR:  -2393.0\n",
      "=== Episode 25 complete ===\n",
      "Avg r:  -7902.0\n",
      "Instant sumR:  -2774.0\n",
      "=== Episode 26 complete ===\n",
      "Avg r:  -7663.18518519\n",
      "Instant sumR:  -1454.0\n",
      "=== Episode 27 complete ===\n",
      "Avg r:  -7413.14285714\n",
      "Instant sumR:  -662.0\n",
      "=== Episode 28 complete ===\n",
      "Avg r:  -7244.51724138\n",
      "Instant sumR:  -2523.0\n",
      "=== Episode 29 complete ===\n",
      "Avg r:  -7040.03333333\n",
      "Instant sumR:  -1110.0\n",
      "=== Episode 30 complete ===\n",
      "Avg r:  -6866.93548387\n",
      "Instant sumR:  -1674.0\n",
      "=== Episode 31 complete ===\n",
      "Avg r:  -6690.15625\n",
      "Instant sumR:  -1210.0\n",
      "=== Episode 32 complete ===\n",
      "Avg r:  -6585.12121212\n",
      "Instant sumR:  -3224.0\n",
      "=== Episode 33 complete ===\n",
      "Avg r:  -6431.26470588\n",
      "Instant sumR:  -1354.0\n",
      "=== Episode 34 complete ===\n",
      "Avg r:  -6281.74285714\n",
      "Instant sumR:  -1198.0\n",
      "=== Episode 35 complete ===\n",
      "Avg r:  -6198.55555556\n",
      "Instant sumR:  -3287.0\n",
      "=== Episode 36 complete ===\n",
      "Avg r:  -6111.05405405\n",
      "Instant sumR:  -2961.0\n",
      "=== Episode 37 complete ===\n",
      "Avg r:  -5974.36842105\n",
      "Instant sumR:  -917.0\n",
      "=== Episode 38 complete ===\n",
      "Avg r:  -5857.74358974\n",
      "Instant sumR:  -1426.0\n",
      "=== Episode 39 complete ===\n",
      "Avg r:  -5726.175\n",
      "Instant sumR:  -595.0\n",
      "=== Episode 40 complete ===\n",
      "Avg r:  -5656.82926829\n",
      "Instant sumR:  -2883.0\n",
      "=== Episode 41 complete ===\n",
      "Avg r:  -5587.85714286\n",
      "Instant sumR:  -2760.0\n",
      "=== Episode 42 complete ===\n",
      "Avg r:  -5480.23255814\n",
      "Instant sumR:  -960.0\n",
      "=== Episode 43 complete ===\n",
      "Avg r:  -5371.27272727\n",
      "Instant sumR:  -686.0\n",
      "=== Episode 44 complete ===\n",
      "Avg r:  -5284.37777778\n",
      "Instant sumR:  -1461.0\n",
      "=== Episode 45 complete ===\n",
      "Avg r:  -5204.04347826\n",
      "Instant sumR:  -1589.0\n",
      "=== Episode 46 complete ===\n",
      "Avg r:  -5101.65957447\n",
      "Instant sumR:  -392.0\n",
      "=== Episode 47 complete ===\n",
      "Avg r:  -5011.75\n",
      "Instant sumR:  -786.0\n",
      "=== Episode 48 complete ===\n",
      "Avg r:  -4940.89795918\n",
      "Instant sumR:  -1540.0\n",
      "=== Episode 49 complete ===\n",
      "Avg r:  -4849.22\n",
      "Instant sumR:  -357.0\n",
      "=== Episode 50 complete ===\n",
      "Avg r:  -4758.8627451\n",
      "Instant sumR:  -241.0\n",
      "=== Episode 51 complete ===\n",
      "Avg r:  -4688.90384615\n",
      "Instant sumR:  -1121.0\n",
      "=== Episode 52 complete ===\n",
      "Avg r:  -4626.66037736\n",
      "Instant sumR:  -1390.0\n",
      "=== Episode 53 complete ===\n",
      "Avg r:  -4560.98148148\n",
      "Instant sumR:  -1080.0\n",
      "=== Episode 54 complete ===\n",
      "Avg r:  -4497.09090909\n",
      "Instant sumR:  -1047.0\n",
      "=== Episode 55 complete ===\n",
      "Avg r:  -4452.01785714\n",
      "Instant sumR:  -1973.0\n",
      "=== Episode 56 complete ===\n",
      "Avg r:  -4393.33333333\n",
      "Instant sumR:  -1107.0\n",
      "=== Episode 57 complete ===\n",
      "Avg r:  -4329.79310345\n",
      "Instant sumR:  -708.0\n",
      "=== Episode 58 complete ===\n",
      "Avg r:  -4272.59322034\n",
      "Instant sumR:  -955.0\n",
      "=== Episode 59 complete ===\n",
      "Avg r:  -4214.35\n",
      "Instant sumR:  -778.0\n",
      "=== Episode 60 complete ===\n",
      "Avg r:  -4177.52459016\n",
      "Instant sumR:  -1968.0\n",
      "=== Episode 61 complete ===\n",
      "Avg r:  -4126.58064516\n",
      "Instant sumR:  -1019.0\n",
      "=== Episode 62 complete ===\n",
      "Avg r:  -4073.80952381\n",
      "Instant sumR:  -802.0\n",
      "=== Episode 63 complete ===\n",
      "Avg r:  -4037.578125\n",
      "Instant sumR:  -1755.0\n",
      "=== Episode 64 complete ===\n",
      "Avg r:  -3983.26153846\n",
      "Instant sumR:  -507.0\n",
      "=== Episode 65 complete ===\n",
      "Avg r:  -3940.51515152\n",
      "Instant sumR:  -1162.0\n",
      "=== Episode 66 complete ===\n",
      "Avg r:  -3906.40298507\n",
      "Instant sumR:  -1655.0\n",
      "=== Episode 67 complete ===\n",
      "Avg r:  -3863.38235294\n",
      "Instant sumR:  -981.0\n",
      "=== Episode 68 complete ===\n",
      "Avg r:  -3828.76811594\n",
      "Instant sumR:  -1475.0\n",
      "=== Episode 69 complete ===\n",
      "Avg r:  -3788.11428571\n",
      "Instant sumR:  -983.0\n",
      "=== Episode 70 complete ===\n",
      "Avg r:  -3765.84507042\n",
      "Instant sumR:  -2207.0\n",
      "=== Episode 71 complete ===\n",
      "Avg r:  -3730.45833333\n",
      "Instant sumR:  -1218.0\n",
      "=== Episode 72 complete ===\n",
      "Avg r:  -3703.63013699\n",
      "Instant sumR:  -1772.0\n",
      "=== Episode 73 complete ===\n",
      "Avg r:  -3659.09459459\n",
      "Instant sumR:  -408.0\n",
      "=== Episode 74 complete ===\n",
      "Avg r:  -3625.24\n",
      "Instant sumR:  -1120.0\n",
      "=== Episode 75 complete ===\n",
      "Avg r:  -3586.69736842\n",
      "Instant sumR:  -696.0\n",
      "=== Episode 76 complete ===\n",
      "Avg r:  -3559.18181818\n",
      "Instant sumR:  -1468.0\n",
      "=== Episode 77 complete ===\n",
      "Avg r:  -3537.51282051\n",
      "Instant sumR:  -1869.0\n",
      "=== Episode 78 complete ===\n",
      "Avg r:  -3496.32911392\n",
      "Instant sumR:  -284.0\n",
      "=== Episode 79 complete ===\n",
      "Avg r:  -3463.8375\n",
      "Instant sumR:  -897.0\n",
      "=== Episode 80 complete ===\n",
      "Avg r:  -3443.49382716\n",
      "Instant sumR:  -1816.0\n",
      "=== Episode 81 complete ===\n",
      "Avg r:  -3409.1097561\n",
      "Instant sumR:  -624.0\n",
      "=== Episode 82 complete ===\n",
      "Avg r:  -3377.90361446\n",
      "Instant sumR:  -819.0\n",
      "=== Episode 83 complete ===\n",
      "Avg r:  -3350.73809524\n",
      "Instant sumR:  -1096.0\n",
      "=== Episode 84 complete ===\n",
      "Avg r:  -3325.08235294\n",
      "Instant sumR:  -1170.0\n",
      "=== Episode 85 complete ===\n",
      "Avg r:  -3295.47674419\n",
      "Instant sumR:  -779.0\n",
      "=== Episode 86 complete ===\n",
      "Avg r:  -3261.95402299\n",
      "Instant sumR:  -379.0\n",
      "=== Episode 87 complete ===\n",
      "Avg r:  -3230.29545455\n",
      "Instant sumR:  -476.0\n",
      "=== Episode 88 complete ===\n",
      "Avg r:  -3201.53932584\n",
      "Instant sumR:  -671.0\n",
      "=== Episode 89 complete ===\n",
      "Avg r:  -3172.54444444\n",
      "Instant sumR:  -592.0\n",
      "=== Episode 90 complete ===\n",
      "Avg r:  -3151.68131868\n",
      "Instant sumR:  -1274.0\n",
      "=== Episode 91 complete ===\n",
      "Avg r:  -3123.41304348\n",
      "Instant sumR:  -551.0\n",
      "=== Episode 92 complete ===\n",
      "Avg r:  -3095.04301075\n",
      "Instant sumR:  -485.0\n",
      "=== Episode 93 complete ===\n",
      "Avg r:  -3067.21276596\n",
      "Instant sumR:  -479.0\n",
      "=== Episode 94 complete ===\n",
      "Avg r:  -3044.84210526\n",
      "Instant sumR:  -942.0\n",
      "=== Episode 95 complete ===\n",
      "Avg r:  -3018.09375\n",
      "Instant sumR:  -477.0\n",
      "=== Episode 96 complete ===\n",
      "Avg r:  -2994.39175258\n",
      "Instant sumR:  -719.0\n",
      "=== Episode 97 complete ===\n",
      "Avg r:  -2970.41836735\n",
      "Instant sumR:  -645.0\n",
      "=== Episode 98 complete ===\n",
      "Avg r:  -2949.76767677\n",
      "Instant sumR:  -926.0\n",
      "=== Episode 99 complete ===\n",
      "Avg r:  -2928.0\n",
      "Instant sumR:  -773.0\n",
      "=== Simulation Complete ===\n",
      "Final score: -2928\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simulation \"\"\"\n",
    "env = gym.make(problem)\n",
    "Q=Q_agente_DNN(2,100,100,3,0.99,0.5,0.0075)\n",
    "T=Target_Q_agente_DNN(2,100,100,3,0.99,0.5,0.0075)\n",
    "Q.construct_model_and_initialize()\n",
    "T.construct_model_and_initialize()\n",
    "T.computacao_eficiente(Q)\n",
    "#T.copiar_pesos(Q)\n",
    "D = deque()\n",
    "tr = threading.Thread(target=toggle_rendering)\n",
    "runflag = True\n",
    "tr.start()\n",
    "\n",
    "\n",
    "try:\n",
    "    for e in xrange(episodes):\n",
    "        o = env.reset()  # returns initial observation\n",
    "        oprev = np.array(o, dtype='float32')\n",
    "        a = Q.actua([oprev])\n",
    "        a_t = np.zeros([nA])\n",
    "        a_t[0] = 1\n",
    "        sumR = 0\n",
    "        costs_hist = []\n",
    "        hsize = 0\n",
    "        hi = 0\n",
    "        for t in xrange(time_lim):\n",
    "\n",
    "            if not runflag:\n",
    "                break\n",
    "\n",
    "            #if render or episodes-e < render_last:\n",
    "            #    env.render()\n",
    "\n",
    "            (o, r, done, info) = env.step(a)\n",
    "            onext = np.array(o, dtype='float32')\n",
    "            if done:\n",
    "                of = oprev\n",
    "            rdeq.append(r)\n",
    "\n",
    "            D.append((oprev,a_t,onext,r,done))\n",
    "            \n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft() \n",
    "            \n",
    "            if (t+1) % C == 0:\n",
    "                T.computacao_eficiente(Q)\n",
    "                #T.copiar_pesos(Q)\n",
    "            \n",
    "            a_t = np.zeros([nA])\n",
    "\n",
    "            \n",
    "            \"\"\" Training the QNet \"\"\"\n",
    "            if len(D) >= batch_size:\n",
    "                minibatch = random.sample(D, batch_size)\n",
    "\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                s_j1_batch = [d[2] for d in minibatch]\n",
    "                r_batch = [d[3] for d in minibatch]\n",
    "                \n",
    "\n",
    "                y_batch = []\n",
    "                readout_j1_batch = T.pred.eval(session=T.sess,feed_dict = {T.x : s_j1_batch})\n",
    "                for i in range(0, len(minibatch)):\n",
    "                    # if terminal only equals reward\n",
    "                    if minibatch[i][4]:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + gamma * np.max(readout_j1_batch[i]))\n",
    "                \n",
    "               \n",
    "                Q.sess.run(Q.optm,feed_dict = {\n",
    "                    Q.future_rewards : y_batch,\n",
    "                    Q.a : a_batch,\n",
    "                    Q.x : s_j_batch})\n",
    "                \n",
    "\n",
    "                if random.random() <= epsilon:\n",
    "                    a = random.randrange(nA)\n",
    "                    a_t[a] = 1\n",
    "                else:\n",
    "                    a = Q.actua([onext])\n",
    "                    a_t[a] = 1\n",
    "                    \n",
    "            else:\n",
    "                a = np.random.choice(nA)\n",
    "                a_t[a] = 1\n",
    "            sumR += r\n",
    "            oprev = onext\n",
    "            if done:\n",
    "                break\n",
    "        Rw.append(sumR)\n",
    "        avgRh.append(np.mean(Rw))\n",
    "\n",
    "        epsilon = epsilon*eps_dec if epsilon > eps_min else eps_min\n",
    "\n",
    "        print \"=== Episode %d complete ===\" % e\n",
    "        print \"Avg r: \", np.mean(Rw)\n",
    "        print \"Instant sumR: \", sumR\n",
    "\n",
    "        if not runflag:\n",
    "            break\n",
    "\n",
    "    print \"=== Simulation Complete ===\"\n",
    "    print \"Final score: %d\" % avgRh[-1]\n",
    "\n",
    "    \"\"\" Visualization of the results \"\"\"\n",
    "    f1 = plt.figure()\n",
    "    spy = f1.add_subplot(111)\n",
    "    spy.plot(avgRh, label='Avg. R over 100 eps.', linewidth=2.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Episodes')\n",
    "    Q.sess.close()\n",
    "    T.sess.close()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    runflag = False\n",
    "    tr.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-0.46852192, -0.00151133], dtype=float32),\n",
       "  array([ 0.,  1.,  0.]),\n",
       "  array([-0.47044444, -0.00192253], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([ -4.66921657e-01,   3.34080920e-04], dtype=float32),\n",
       "  array([ 0.,  1.,  0.]),\n",
       "  array([ -4.67010587e-01,  -8.89523071e-05], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([-0.47176394, -0.0013195 ], dtype=float32),\n",
       "  array([ 0.,  1.,  0.]),\n",
       "  array([-0.47347066, -0.00170669], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([-0.47044444, -0.00192253], dtype=float32),\n",
       "  array([ 0.,  0.,  1.]),\n",
       "  array([-0.47176394, -0.0013195 ], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([ -4.67255712e-01,  -2.45355535e-04], dtype=float32),\n",
       "  array([ 0.,  0.,  1.]),\n",
       "  array([ -4.66921657e-01,   3.34080920e-04], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([-0.46818739,  0.0005907 ], dtype=float32),\n",
       "  array([ 0.,  0.,  1.]),\n",
       "  array([-0.46701038,  0.00117702], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([-0.46701038,  0.00117702], dtype=float32),\n",
       "  array([ 1.,  0.,  0.]),\n",
       "  array([ -4.67255712e-01,  -2.45355535e-04], dtype=float32),\n",
       "  -1.0,\n",
       "  False),\n",
       " (array([-0.47347066, -0.00170669], dtype=float32),\n",
       "  array([ 0.,  0.,  1.]),\n",
       "  array([-0.47455189, -0.00108124], dtype=float32),\n",
       "  -1.0,\n",
       "  False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17822835 -0.50298917 -0.50319928 -0.50297076 -0.50292581 -0.15190996\n",
      " -0.50289959 -0.15183537]\n"
     ]
    }
   ],
   "source": [
    "y=Q.sess.run(Q.y,feed_dict = {\n",
    "                Q.future_rewards : y_batch,\n",
    "                Q.a : a_batch,\n",
    "                Q.x : s_j_batch})\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15540685  0.17972887 -0.04821161]\n",
      " [ 0.15538457  0.17974788 -0.04821653]\n",
      " [ 0.15554243  0.17982095 -0.04816839]\n",
      " [ 0.15536901  0.1797404  -0.04822126]\n",
      " [ 0.15533277  0.1797232  -0.04823238]\n",
      " [ 0.15539819  0.1797545  -0.04821236]\n",
      " [ 0.15531226  0.17971349 -0.04823864]\n",
      " [ 0.15554881  0.17982459 -0.04816639]]\n"
     ]
    }
   ],
   "source": [
    "readout_j1_batch = T.pred.eval(session=T.sess,feed_dict = {T.x : s_j1_batch})\n",
    "print readout_j1_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(minibatch)):\n",
    "                    # if terminal only equals reward\n",
    "                    if minibatch[i][4]:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + gamma * np.max(readout_j1_batch[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.82206842303276062, -0.82204959928989407, -0.82197725474834438, -0.82205700486898425, -0.82207402884960179, -0.82204304933547978, -0.82208364725112915, -0.82197365522384647, -0.82206842303276062, -0.82204959928989407, -0.82197725474834438, -0.82205700486898425, -0.82207402884960179, -0.82204304933547978, -0.82208364725112915, -0.82197365522384647, -0.82206842303276062, -0.82204959928989407, -0.82197725474834438, -0.82205700486898425, -0.82207402884960179, -0.82204304933547978, -0.82208364725112915, -0.82197365522384647]\n"
     ]
    }
   ],
   "source": [
    "print y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15183972 -0.50319153 -0.17822835]\n",
      " [-0.15188438 -0.50298917 -0.17821157]\n",
      " [-0.15183708 -0.50319928 -0.17822964]\n",
      " [-0.15188892 -0.50297076 -0.17820966]\n",
      " [-0.15189964 -0.50292581 -0.1782054 ]\n",
      " [-0.15190996 -0.50305265 -0.17819068]\n",
      " [-0.15190576 -0.50289959 -0.17820299]\n",
      " [-0.15183537 -0.5032025  -0.17823055]]\n"
     ]
    }
   ],
   "source": [
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  1.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 1.,  0.,  0.]),\n",
       " array([ 0.,  1.,  0.]),\n",
       " array([ 1.,  0.,  0.])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17822835, -0.50298917, -0.50319928, -0.50297076, -0.50292581,\n",
       "       -0.15190996, -0.50289959, -0.15183537], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51449412 -0.2868346  -0.32623082]\n",
      " [-0.51469588 -0.28694007 -0.32635134]\n",
      " [-0.51538187 -0.28726935 -0.32675299]\n",
      " [-0.5151791  -0.28719196 -0.32663488]\n",
      " [-0.51399523 -0.28656989 -0.32593334]\n",
      " [-0.51437277 -0.28678417 -0.32615682]\n",
      " [-0.51565862 -0.28739598 -0.32691297]\n",
      " [-0.51381671 -0.28646675 -0.32582784]]\n",
      "[[-0.51449412 -0.2868346  -0.32623082]\n",
      " [-0.51469588 -0.28694007 -0.32635134]\n",
      " [-0.51538187 -0.28726935 -0.32675299]\n",
      " [-0.5151791  -0.28719196 -0.32663488]\n",
      " [-0.51399523 -0.28656989 -0.32593334]\n",
      " [-0.51437277 -0.28678417 -0.32615682]\n",
      " [-0.51565862 -0.28739598 -0.32691297]\n",
      " [-0.51381671 -0.28646675 -0.32582784]]\n"
     ]
    }
   ],
   "source": [
    "y= Q.sess.run(Q.pred, feed_dict={Q.x: s_j_batch})\n",
    "y_t = T.sess.run(T.pred, feed_dict={T.x: s_j_batch})\n",
    "print y\n",
    "print y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_batch = []\n",
    "readout_j1_batch = T.pred.eval(session=T.sess,feed_dict = {T.x : s_j1_batch})\n",
    "for i in range(0, len(minibatch)):\n",
    "# if terminal only equals reward\n",
    "    if minibatch[i][4]:\n",
    "        y_batch.append(r_batch[i])\n",
    "    else:\n",
    "        y_batch.append(r_batch[i] + gamma * np.max(readout_j1_batch[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T.computacao_eficiente(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11842299,  0.04439117,  0.16220994,  0.01512942,  0.06441902,\n",
       "        -0.2058327 ,  0.15205504,  0.02889177,  0.05861564, -0.17261411,\n",
       "         0.18193264, -0.15791897, -0.00820857,  0.17591423, -0.11611249,\n",
       "        -0.01017192,  0.10892375,  0.08214349, -0.24542077,  0.06357118,\n",
       "         0.11206867, -0.17252877, -0.13303795, -0.14100067, -0.02839502,\n",
       "        -0.08539977,  0.11517923, -0.04498134,  0.03216136,  0.26617774,\n",
       "         0.06585611,  0.05631948, -0.00068399,  0.11056302, -0.09624262,\n",
       "        -0.08573872,  0.19438291,  0.02016669,  0.06718251, -0.09028285,\n",
       "        -0.0637907 , -0.03176225, -0.05000495,  0.10110813,  0.07702964,\n",
       "         0.17069994,  0.0562891 ,  0.09567304, -0.17517808,  0.10179514,\n",
       "         0.05129729,  0.03390796,  0.0892757 ,  0.00649925, -0.01418992,\n",
       "         0.0712105 ,  0.00742582, -0.17233565,  0.08699498, -0.16153537,\n",
       "        -0.14114662,  0.04416197,  0.0282966 ,  0.03731935, -0.07315317,\n",
       "        -0.06846406,  0.16159391,  0.00313542, -0.04981846, -0.03304964,\n",
       "         0.13803639,  0.26024517, -0.13232613, -0.08734971, -0.07878309,\n",
       "        -0.0020598 ,  0.06834159,  0.04049413, -0.03053622,  0.11654495,\n",
       "         0.02790278, -0.00668048,  0.06480455, -0.06956553, -0.05066913,\n",
       "         0.03824645, -0.02589091,  0.02128807,  0.0042293 ,  0.06059259,\n",
       "        -0.05362406, -0.05174045,  0.07191209,  0.05223404, -0.06055307,\n",
       "         0.01787269,  0.0165705 , -0.02562393,  0.1065855 , -0.12509444],\n",
       "       [-0.06059456, -0.03622531,  0.21291955, -0.13332173, -0.02872712,\n",
       "         0.00677858, -0.09946752,  0.03438658, -0.09656356,  0.03185647,\n",
       "         0.12870626,  0.0396951 , -0.02963849, -0.02929699,  0.20458056,\n",
       "        -0.01664845,  0.06000701, -0.05276005,  0.04427287, -0.0386742 ,\n",
       "         0.00114669,  0.01413414, -0.1462903 , -0.05770412, -0.12683852,\n",
       "         0.06900924, -0.05789369, -0.12691252,  0.11082899, -0.13165306,\n",
       "         0.0013185 ,  0.01285139,  0.01890216, -0.02652031,  0.06442972,\n",
       "        -0.06685974,  0.12554502, -0.03032825, -0.09496142, -0.17996438,\n",
       "        -0.09471832,  0.06563428, -0.05886763,  0.02050532, -0.05318724,\n",
       "         0.11812001,  0.06231995, -0.02063646,  0.005782  ,  0.03210183,\n",
       "         0.08898596,  0.03863731,  0.05430443, -0.0267025 , -0.11348312,\n",
       "         0.05914348, -0.04020587,  0.06736767,  0.0935801 , -0.13072436,\n",
       "         0.01951002,  0.0151349 ,  0.01224529, -0.09933798,  0.04319883,\n",
       "         0.00366749, -0.16782357, -0.00216459,  0.21537769, -0.13206704,\n",
       "        -0.0385026 , -0.03059318, -0.06684492,  0.00072092, -0.0786035 ,\n",
       "        -0.07624353,  0.05762113, -0.09226611, -0.01294896,  0.03208702,\n",
       "        -0.00927255, -0.09904322, -0.07227754,  0.15278818, -0.06363869,\n",
       "         0.13135277,  0.09529173,  0.08816448, -0.15345217, -0.05893269,\n",
       "        -0.13430728,  0.02966163, -0.04188132, -0.0364344 ,  0.17110023,\n",
       "         0.01271234, -0.06158821, -0.26466975, -0.0334678 ,  0.09214225]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.sess.run(Q.weights['h1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09593325,  0.06577849,  0.16220994,  0.00877897,  0.06441902,\n",
       "        -0.2058327 ,  0.15205504,  0.00950908,  0.05861564, -0.15026949,\n",
       "         0.18193264, -0.16858712, -0.00905997,  0.17591423, -0.10095793,\n",
       "        -0.01017192,  0.13051046,  0.08214349, -0.24693875,  0.08607934,\n",
       "         0.11206867, -0.15008587, -0.11109424, -0.11856753, -0.04807801,\n",
       "        -0.06388431,  0.11517923, -0.02398199,  0.01023276,  0.26617774,\n",
       "         0.0456562 ,  0.07851242, -0.0133181 ,  0.11056302, -0.11705349,\n",
       "        -0.08387447,  0.19438291,  0.02016669,  0.08960155, -0.09713677,\n",
       "        -0.07537597, -0.03176225, -0.02784461,  0.10110813,  0.07702964,\n",
       "         0.17069994,  0.0562891 ,  0.09567304, -0.15354276,  0.10179514,\n",
       "         0.03488758,  0.05632363,  0.0892757 ,  0.02884839, -0.02712709,\n",
       "         0.0712105 ,  0.00742582, -0.19226597,  0.10942382, -0.13944004,\n",
       "        -0.11909824,  0.06528366,  0.0282966 ,  0.05918797, -0.05121944,\n",
       "        -0.04766155,  0.16159391, -0.0051396 , -0.04981846, -0.03304964,\n",
       "         0.13803639,  0.26024517, -0.15327831, -0.08734971, -0.07878309,\n",
       "        -0.02050861,  0.06834159,  0.02026047, -0.03053622,  0.11654495,\n",
       "         0.04929503, -0.02705213,  0.06480455, -0.06956553, -0.05066913,\n",
       "         0.03824645, -0.02589091,  0.02128807,  0.0042293 ,  0.06059259,\n",
       "        -0.03122755, -0.05174045,  0.07191209,  0.05223404, -0.06055307,\n",
       "         0.03825767,  0.03906941, -0.02562393,  0.1065855 , -0.10352173],\n",
       "       [-0.03890695, -0.01473293,  0.21291955, -0.11332362, -0.02872712,\n",
       "         0.00677858, -0.09946752,  0.01296839, -0.09656356,  0.05418612,\n",
       "         0.12870626,  0.033766  , -0.03479499, -0.02929699,  0.22583143,\n",
       "        -0.01664845,  0.08137377, -0.05276005,  0.03026192, -0.01635143,\n",
       "         0.00114669,  0.00767713, -0.12506312, -0.03562535, -0.14888154,\n",
       "         0.091421  , -0.05789369, -0.10493066,  0.08845109, -0.13165306,\n",
       "        -0.01906301,  0.0349116 ,  0.02236184, -0.02652031,  0.04248219,\n",
       "        -0.04521619,  0.12554502, -0.03032825, -0.07260269, -0.19867504,\n",
       "        -0.07288878,  0.06563428, -0.05826462,  0.02050532, -0.05318724,\n",
       "         0.11812001,  0.06231995, -0.02063646,  0.02649476,  0.03210183,\n",
       "         0.0727386 ,  0.06103673,  0.05430443, -0.00422807, -0.11810016,\n",
       "         0.05914348, -0.04020587,  0.04673639,  0.11604434, -0.14525175,\n",
       "         0.04184924,  0.03610028,  0.01224529, -0.07870976,  0.02958908,\n",
       "         0.00042218, -0.16782357,  0.02003212,  0.21537769, -0.13206704,\n",
       "        -0.0385026 , -0.03059318, -0.04740992,  0.00072092, -0.0786035 ,\n",
       "        -0.05579106,  0.05762113, -0.10364385, -0.01294896,  0.03208702,\n",
       "         0.01183669, -0.11960496, -0.07227754,  0.15278818, -0.06363869,\n",
       "         0.13135277,  0.09529173,  0.08816448, -0.15345217, -0.05893269,\n",
       "        -0.11264761,  0.02966163, -0.04188132, -0.0364344 ,  0.17110023,\n",
       "         0.03471941, -0.03910142, -0.26466975, -0.0334678 ,  0.1132872 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sess.run(T.weights['h1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
